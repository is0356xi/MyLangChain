{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8328b63",
   "metadata": {},
   "source": [
    "# 関数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import StoppingCriteria\n",
    "from transformers import StoppingCriteriaList\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import torch\n",
    "\n",
    "def model_setup(model_id:str):\n",
    "    # モデル&トークナイザーのダウンロード\n",
    "    print(f\"!!! Downloading Model from {model_id} !!!\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def pipeline_setup(model, tokenizer, isGPU:bool, **kwargs) -> HuggingFacePipeline:\n",
    "    # GPUの確認\n",
    "    if isGPU:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"\\n!!! current device is {device} !!!\\n\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # GPUにモデルを展開する際に必要な引数を追加\n",
    "        device = 0\n",
    "        framework = 'pt'\n",
    "    else:\n",
    "        device = -1\n",
    "        framework = None\n",
    "        \n",
    "        \n",
    "    # パイプラインの作成\n",
    "    task = \"text-generation\"\n",
    "    pipe = pipeline(\n",
    "        task,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        framework=framework,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # LLMs: LangChainで利用可能な形に変換\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "    print(\"!!! Pipeline Setup Completed !!!\")\n",
    "    \n",
    "    return llm\n",
    "\n",
    "\n",
    "\n",
    "# Stopの条件を設定するクラスを作成 (StoppingCriteriaを継承する)\n",
    "class MyStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_str, num_iter, tokenizer, isGPU):\n",
    "        if isGPU:\n",
    "            self.stop_token_ids = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"].to('cuda')\n",
    "            self.stop_token_ids_iter = tokenizer(stop_str*2, return_tensors='pt')[\"input_ids\"].to('cuda')\n",
    "        else:\n",
    "            self.stop_token_ids = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"]\n",
    "            self.stop_token_ids_iter = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"]\n",
    "            \n",
    "        self.num_iter = num_iter\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, input_ids:torch.LongTensor, score:torch.FloatTensor, **kwargs):\n",
    "        # 出力の最後尾の文字列とstop_strが一致した回数\n",
    "        match_count = 0\n",
    "        \n",
    "        # 出力文字列を最後尾から順に、num_iterで指定された要素数だけ処理する\n",
    "        for i in range(1, self.num_iter+1): \n",
    "            input_id = input_ids[0][-i]\n",
    "            stop_id = self.stop_token_ids[0][0]\n",
    "            stop_iter_id = self.stop_token_ids_iter[0][0]\n",
    "            \n",
    "            # 対象文字列とstop_strが一致した場合、カウントを増やす\n",
    "            if input_id == stop_id:\n",
    "                match_count += 1\n",
    "            \n",
    "        # \\nが2回続いた場合、または\\n\\nが現れた場合、generate()をStopする\n",
    "        if match_count == self.num_iter or input_id == stop_iter_id:\n",
    "            isStop = True\n",
    "            # print(f\"!!! Generate() Stopped !!!\\n!!!!!!!!!\\n{self.tokenizer.decode(input_ids[0])} \\n!!!!!!!!!\")\n",
    "        else:\n",
    "            isStop = False\n",
    "        return isStop\n",
    "    \n",
    "    \n",
    "def chat_chain_setup(template, llm) -> LLMChain:\n",
    "    # Memory: メモリ上に会話を記録する設定\n",
    "    memory_key = \"chat_history\"\n",
    "    memory = ConversationBufferMemory(memory_key=memory_key, ai_prefix=\"\")\n",
    "    \n",
    "    # Prompts: プロンプトを作成\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"chat_history\", \"input\"])\n",
    "\n",
    "    # Chains: プロンプト&モデル&メモリをチェーンに登録\n",
    "    llm_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        memory=memory\n",
    "    )\n",
    "    \n",
    "    return llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22682560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルをダウンロード\n",
    "model_id = \"andreaskoepf/pythia-1.4b-gpt4all-pretrain\"\n",
    "model, tokenizer = model_setup(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc793f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopの条件式に用いる文字と、その文字が何回続いたらStopするかを指定\n",
    "stop_str = \"\\n\"\n",
    "num_iter = 2  # \\nが2回繰り返された場合、generate()をstopする\n",
    "\n",
    "# StoppingCriteriaListクラスのインスタンスを生成\n",
    "stopcriteria_list = StoppingCriteriaList([MyStoppingCriteria(stop_str, num_iter, tokenizer, isGPU=True)])\n",
    "print(stopcriteria_list)\n",
    "\n",
    "# HuggingFacePipelineを作成\n",
    "model_args = {\"temperature\":0.1, \"max_length\": 256, \"stopping_criteria\": stopcriteria_list}\n",
    "llm = pipeline_setup(model=model, tokenizer=tokenizer, isGPU=True, **model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64905af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロンプトテンプレートを作成\n",
    "template = \"\"\"\n",
    "You are an AI who responds to user Input.\n",
    "Please provide an answer to the human's question.\n",
    "Additonaly, you are having a conversation with a human based on past interactions.\n",
    "\n",
    "### Answer Sample\n",
    "Human: Hi!\n",
    "AI: Hi, nice to meet you.\n",
    "\n",
    "### Past Interactions\n",
    "{chat_history}\n",
    "\n",
    "### \n",
    "Human:{input}\n",
    "\"\"\"\n",
    "\n",
    "# Chat用のチェーンを作成\n",
    "llm_chain = chat_chain_setup(template, llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48b60f6b",
   "metadata": {},
   "source": [
    "# 手順①: シンプルなQAをする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの確認\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n!!! current device is {device} !!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのダウンロード\n",
    "model_id = \"andreaskoepf/pythia-1.4b-gpt4all-pretrain\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19800c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パイプラインの作成\n",
    "task = \"text-generation\"\n",
    "pipe = pipeline(\n",
    "    task,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    framework='pt',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "\n",
    "# LLMs: LangChainで利用可能な形に変換\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a394f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts: プロンプトを作成\n",
    "template = \"\"\"You are an assistant who responds to user Input. \\nPlease provide an answer to the user's question, as shown in the following example.\\n\\nExample:\\nQuestion: What is the height of Tokyo Tower in meters?\\nAnswer: The height of Tokyo Tower is 333 meters.\\n\\n###\\n\\nInput:\\n{question}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Chains: チェーンに登録\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 質問を投げる\n",
    "# question = \"How can I get end of the list in Python?Take an example of Python Code.\"\n",
    "question = input(\"Enter your question\")\n",
    "generated_text = llm_chain.run(question)\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a4a4b74",
   "metadata": {},
   "source": [
    "# 手順②: 会話を記録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d156cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Memory: メモリ上に会話を記録する設定\n",
    "memory_key = \"chat_history\"\n",
    "memory = ConversationBufferMemory(memory_key=memory_key, ai_prefix=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb02dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts: プロンプトを作成。会話履歴もinput_variablesとして指定する\n",
    "template = \"\"\"\n",
    "You are an AI who responds to user Input.\n",
    "Please provide an answer to the human's question.\n",
    "Additonaly, you are having a conversation with a human based on past interactions.\n",
    "\n",
    "### Answer Sample\n",
    "Human: Hi!\n",
    "AI: Hi, nice to meet you.\n",
    "\n",
    "### Past Interactions\n",
    "{chat_history}\n",
    "\n",
    "### \n",
    "Human:{input}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"chat_history\", \"input\"])\n",
    "\n",
    "# Chains: プロンプト&モデル&メモリをチェーンに登録\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 実行①\n",
    "user_input = \"What is the Japanese word for mountain？\"\n",
    "response = llm_chain.predict(input=user_input)\n",
    "print(response)\n",
    "\n",
    "# 履歴表示\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67620d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行②\n",
    "user_input = \"Hi, I`m Matthew. Nice to meet you.\"\n",
    "response = llm_chain.predict(input=user_input)\n",
    "print(response)\n",
    "\n",
    "# 履歴表示\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行③\n",
    "user_input = \"Please call my name.\"\n",
    "response = llm_chain.predict(input=user_input)\n",
    "print(response)\n",
    "\n",
    "# 履歴表示\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria\n",
    "from transformers import StoppingCriteriaList\n",
    "\n",
    "# Stopの条件式に用いる文字と、その文字が何回続いたらStopするかを指定\n",
    "stop_str = \"\\n\"\n",
    "num_iter = 2  # \\nが2回繰り返された、または\\n\\nが現れた場合にgenerate()をstopする\n",
    "\n",
    "\n",
    "# Stopの条件を設定するクラスを作成 (StoppingCriteriaを継承する)\n",
    "class MyStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_str, num_iter, tokenizer, isGPU):\n",
    "        if isGPU:\n",
    "            self.stop_token_ids = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"].to('cuda')\n",
    "            self.stop_token_ids_iter = tokenizer(stop_str*2, return_tensors='pt')[\"input_ids\"].to('cuda')\n",
    "        else:\n",
    "            self.stop_token_ids = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"]\n",
    "            self.stop_token_ids_iter = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"]\n",
    "            \n",
    "        self.num_iter = num_iter\n",
    "        self.tokenizer = tokenizer\n",
    "#         self.num_stop = 0\n",
    "        self.num_stop = 1\n",
    "        \n",
    "    def __call__(self, input_ids:torch.LongTensor, score:torch.FloatTensor, **kwargs):\n",
    "        # 出力の最後尾の文字列とstop_strが一致した回数\n",
    "        match_count = 0\n",
    "        print(tokenizer.decode(input_ids[0]))\n",
    "        print(input_ids)\n",
    "        \n",
    "        # 出力文字列を最後尾から順に、num_iterで指定された要素数だけ処理する\n",
    "        for i in range(1, self.num_iter+1): \n",
    "            input_id = input_ids[0][-i]\n",
    "            stop_id = self.stop_token_ids[0][0]\n",
    "            stop_iter_id = self.stop_token_ids_iter[0][0]\n",
    "            \n",
    "            # 対象文字列とstop_strが一致した場合、カウントを増やす\n",
    "            if input_id == stop_id:\n",
    "                match_count += 1\n",
    "            \n",
    "        \n",
    "        # モデルが最初に\\n\\nを出力する仕様の場合、Stop条件を1回だけ無視する\n",
    "        if (match_count == num_iter) and self.num_stop == 0:\n",
    "            isStop = False\n",
    "            self.num_stop += 1\n",
    "            print(\"!!! FirstStop was ignored!!!\")\n",
    "        # \\nが2回続いた場合、または\\n\\nが現れた場合、generate()をStopする\n",
    "        elif match_count == num_iter or input_id == stop_iter_id:\n",
    "            isStop = True\n",
    "#             self.num_stop = 0\n",
    "            print(f\"!!! Generate() Stopped !!!\\n!!!!!!!!!\\n{self.tokenizer.decode(input_ids[0])} \\n!!!!!!!!!\")\n",
    "        else:\n",
    "            isStop = False\n",
    "        return isStop\n",
    "\n",
    "\n",
    "# StoppingCriteriaListクラスのインスタンスを生成\n",
    "stopcriteria_list = StoppingCriteriaList([MyStoppingCriteria(stop_str, num_iter, tokenizer, isGPU=True)])\n",
    "print(stopcriteria_list)\n",
    "\n",
    "# HuggingFacePipelineを作成\n",
    "model_args = {\"temperature\":0.1, \"max_length\": 256, \"stopping_criteria\": stopcriteria_list}\n",
    "llm = pipeline_setup(model=model, tokenizer=tokenizer, isGPU=True, **model_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8affabe5",
   "metadata": {},
   "source": [
    "# 手順③チャットボット化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロンプトテンプレートを作成\n",
    "template = \"\"\"\n",
    "You are an AI who responds to user Input.\n",
    "Please provide an answer to the human's question.\n",
    "Additonaly, you are having a conversation with a human based on past interactions.\n",
    "\n",
    "### Answer Sample\n",
    "Human: Hi!\n",
    "AI: Hi, nice to meet you.\n",
    "\n",
    "### Past Interactions\n",
    "{chat_history}\n",
    "\n",
    "###\n",
    "Human:{input}\n",
    "\"\"\"\n",
    "\n",
    "# Chat用のチェーンを作成\n",
    "llm_chain = chat_chain_setup(template, llm)\n",
    "\n",
    "# チャット形式\n",
    "while True:\n",
    "    user_input = input(\"\\n> \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    else:\n",
    "        response = llm_chain.predict(input=user_input)\n",
    "        print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a303a9b",
   "metadata": {},
   "source": [
    "# その他、テストなど"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StoppingCriterial実装のためのテスト\n",
    "test1 = \"This is Test\\n\"\n",
    "stop = \"\\n\"\n",
    "\n",
    "test1_ids = tokenizer(test1, return_tensors='pt')[\"input_ids\"]\n",
    "stop_token_ids = tokenizer(stop*2, return_tensors='pt')[\"input_ids\"]\n",
    "\n",
    "num_iter = 2\n",
    "print(test1_ids, stop_token_ids)\n",
    "for i in range(1, num_iter+1):\n",
    "    print(test1_ids1[-1][-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Memory: メモリ上に会話を記録する設定\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "template = \"\"\"\n",
    "You provides lots of specific information based on the context of the conversation. \n",
    "Here is a conversation between a human and an AI.\n",
    "Human: Hi!\n",
    "AI: Hi, nice to meet you.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(template),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=True)\n",
    "\n",
    "user_input = \"Hi, I`m Matthew.\"\n",
    "response = conversation.predict(input=user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9602ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
